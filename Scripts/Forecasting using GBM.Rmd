---
title: "Forecasting using GBM"
author: "David Torgerson & Danny Morris"
date: "3/19/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This project looks at trying to forecast crime rates in Buffalo, NY. The data used for this analysis can be located here:https://data.buffalony.gov/Public-Safety/Crime-Incidents/d6g9-xbgu . The analysis involves multiple data science practices including, data extraction, data cleaning, model building and model tuning. 

# Libraries Needed
```{Libraries}
library(tidyverse)
library(lubridate)
library(stringr)
library(tibble)
library(purrr)
library(rsample)
library(tis)
```

# Reading in the Data

For this particular analysis, I had downloaded the data as an excel file from the buffalony.gov website and saved it in a memorable folder.

```{Data}
crime = read_csv("Data/Crime_Incidents.csv")
```

# Cleaning the Data

There were some minor issues with the data that needed to be fixed. Some dates needed to be corrected, missing values filled in, need consistent formatting throughout a variable. After the initial round of cleaning, we noticed that there were still some problems. We had some outlier years that needed to be removed, needed to standardize the incident variables, and then remove any incidents that have very few observations.

```{Data Cleaning}

crime_clean = crime %>%
  mutate(address_1 = toupper(address_1)) %>%
  mutate(incident_datetime = mdy_hms(incident_datetime)) %>%
  mutate(incident_type_primary = toupper(incident_type_primary)) %>%
  mutate(day_of_week = str_to_sentence(day_of_week))

crime_since_2009 = crime_clean %>%
  filter(year(incident_datetime) >= '2009') %>%
  mutate(incident = case_when(
    incident_type_primary %in% c('THEFT OF SERVICES','THEFT OF VEHICLE') ~ 'THEFT', #Aggregating crimes
    incident_type_primary %in% c('AGG ASSAULT ON P/OFFICER', 'AGGR ASSAULT') ~ 'ASSAULT', #Aggregating crimes
    TRUE ~ incident_type_primary)) %>%
  filter(!incident %in% c('BREAKING & ENTERING','HOMICIDE','CRIM NEGLIGENT HOMICIDE',
                       'MANSLAUGHTER','OTHER SEXUAL OFFENSE','SEXUAL ASSAULT')) 
```

# Prepping the data for modeling

Once the data has been cleaned, we started prepping the data for modeling. We had to make sure there was no skip in dates and map a dates data frame to our clean data so we can make holiday indicator features. These holiday indicators will be used to determine feature importance.
```{Data Prep}
seq_dates = seq.Date(
  from = as.Date("2009-01-01"),
  to = as.Date("2021-01-24"),
  by = 1) %>%
  enframe(name = NULL, value = "incident_date")

all_daily_incidents = daily_incident_counts %>%
  split(.$incident) %>%
  map(., function(i) {
    left_join(
      x = seq_dates,
      y = i,
      by = "incident_date"
    ) %>%
      fill(incident, .direction = "downup") %>%
      replace_na(list(n = 0))
  }) %>%
  bind_rows()
#The code above splits the dataframe by incidents map the dates together
#Filling in all the missing dates with a count of 0
#Assumption: If date is missing from original data, that incident did not occur (count of 0)

#Extracting the other date-based data features
complete_daily_incidents = all_daily_incidents %>%
  mutate(month = month(incident_date)) %>%
  mutate(year = year(incident_date)) %>%
  mutate(week = week(incident_date)) %>%
  mutate(weekday = wday(incident_date)) %>%
  rename(crime_count = n)

#Getting Bank holidays from 2009 to 2021 to incorporate
bank_holidays = holidays(seq(2009,2021, 1)) %>%
  enframe(name = 'Holiday', value = 'Date') %>%
  mutate(New_Date = as.Date(as.character(Date), '%Y%m%d')) %>%
  mutate(is_Holiday = 1)

#Joining with crime data and creating holiday dummy variables.
complete_incidents_with_holidays = left_join(complete_daily_incidents,
                                             bank_holidays,
                                             by = c("incident_date" = "New_Date")) %>%
  select(-Date) %>%
  mutate(Holiday = ifelse(is.na(Holiday), "None",Holiday)) %>%
  spread(Holiday, is_Holiday) %>% 
  mutate(Easter = isEaster(incident_date)) %>%
  mutate(Easter = ifelse(Easter == TRUE, 1, NA)) %>%
  select(-None) %>%
  replace(is.na(.),0) 
  
#Adding a 'Holidays' tag to filter out Holidays to compare models
tag_columns = function(x, prefix) {
  paste(prefix, x, sep = "_")
}

complete_incidents_with_holiday_tag = complete_incidents_with_holidays %>%
  rename_at(vars(-incident_date:-weekday), list(~tag_columns(x = ., prefix = 'Holiday')))

recent_daily_incidents = complete_incidents_with_holiday_tag %>% #Was complete_daily_incidents
  filter(incident_date >= '2015-01-01')
```

# Training and Testing Data

When building a forecasting model, we needed to separate out a training and testing data set. 

```{Training/Testing data}

recent_daily_incidents_wide = recent_daily_incidents %>%
  spread(incident, crime_count)

rocv_by_crime2 = recent_daily_incidents_wide %>%
  rolling_origin(
    data = .,
    initial = (365*4),
    assess = 14,
    cumulative = TRUE,
    skip = 14 
  )

train_test_splits = map(rocv_by_crime2$splits, function(split){
  train = analysis(split) %>%
    gather(incident, crime_count, -incident_date, -month, -year, -week, -weekday, -Holiday_Christmas:-Holiday_Easter)

  test = assessment(split) %>%
    gather(incident, crime_count, -incident_date, -month, -year, -week, -weekday, -Holiday_Christmas:-Holiday_Easter)
  
  out = list(train = train, test = test)
  
  return(out)
})

```

# Model Implementation

Next, we used h2o to build our gradient-boosted machine (GBM) model. This model performed the best against standard linear regression and random forest models. The GBM is being tested against a "simple model". Our Simple model states that we'll use the information from the last 14 days to make a prediction about what will happen next. After comparing the two methods, the GBM performs better on 6/9 of the incidents.

```{Model Building}

library(h2o)

h2o.init()

rocv_models = map(train_test_splits, function(x){
  
  train = x$train %>%
    mutate_at(vars(month:incident), list(as.factor))
  
  test = x$test %>%
    mutate_at(vars(month:incident), list(as.factor))
  
  features = train %>%
    select(-crime_count, -incident_date) %>%
    colnames()
  
  train_h2o = as.h2o(train)
  
  test_h2o = as.h2o(test)
  
  target = "crime_count"

  model = h2o.gbm(
    x = features,
    y = target,
    training_frame = train_h2o,
    validation_frame = test_h2o,
    ntrees = 50,
    learn_rate = 0.1,
    max_depth = 5,
    categorical_encoding = "AUTO",
    distribution = "poisson" 
)
  
  #Simple Predictions Calculation
  simple_model = train %>%
    arrange(incident, incident_date) %>% 
    group_by(incident) %>%
    slice(tail(row_number(), 14)) %>%
    ungroup() %>%
    mutate(incident_date = incident_date + days(14)) %>%
    select(incident_date, incident, Simple = crime_count)
  
  #Predictions from GBM Model
  predictions = h2o.predict(model, newdata = test_h2o) %>%
    as.vector()
  
  #MASE Calculation
  MASE_by_incident = test %>%
    mutate(Predictions = predictions) %>% 
    inner_join(simple_model, by = c("incident","incident_date")) %>%
    group_by(incident) %>%
    summarise(
      GBM_MAE = mean(abs(Predictions - crime_count)),
      Simple_MAE = mean(abs(Simple - crime_count))
    ) %>%
    mutate(MASE = GBM_MAE/Simple_MAE)
  
  #Clean out the h2o cluster.
  h2o.removeAll()
  
  #Reporting out Metrics
  out = MASE_by_incident
  
  return(out)
})

rocv_df = map2(rocv_models, seq_along(rocv_models), function(model, idx){
  
  out = model %>% mutate(rocv_idx = idx)
  
  return(out)
}) %>%
  bind_rows()

rocv_df %>%
  group_by(incident) %>%
  filter(MASE != 'Inf') %>%
  summarise(N = n(),
            Avg_MASE = mean(MASE),
            SD_MASE = sd(MASE),
            Lower_CI = t.test(MASE)$conf.int[1],
            Upper_CI = t.test(MASE)$conf.int[2])
```

# Model Improvements - Hyperparameter Tuning

After running the model, we tried to improve our model by hyperparameter tuning with grid search. Doing this allows the model to run through several different input values and then we can look at the performance of each set of parameters. 

```{Hyperparameter Tuning}
library(h2o)

h2o.init()

rocv_tuning = map(train_test_splits, function(x){
  
  train = x$train %>%
    mutate_at(vars(month:incident), list(as.factor))
  
  test = x$test %>%
    mutate_at(vars(month:incident), list(as.factor))
  
  features = train %>%
    select(-crime_count, -incident_date) %>%
    colnames()
  
  train_h2o = as.h2o(train)
  
  test_h2o = as.h2o(test)
  
  target = "crime_count"
  
  gbm_params = list(
    learn_rate = c(0.01,0.1),
    max_depth = c(3,5,9),
    sample_rate = c(0.8,1.0),
    col_sample_rate = c(0.2,0.5,1.0)
  )
  
  gbm_grid = h2o.grid("gbm", x = features, y = target,
                      training_frame = train_h2o,
                      validation_frame = test_h2o,
                      ntrees = 15,
                      seed = 1,
                      hyper_params = gbm_params)
  
  gbm_grid_sort = h2o.getGrid(grid_id = gbm_grid@grid_id,
                              sort_by = "rmse",
                              decreasing = TRUE)
  
  best_model_id = gbm_grid_sort@model_ids[[1]]
  
  best_gbm = h2o.getModel(best_model_id)
  
  best_params = gbm_grid_sort@summary_table %>%
    as_tibble() %>%
    mutate_at(vars(-model_ids), list(as.numeric)) %>%
    filter(model_ids == best_model_id)
  
  worst_params = gbm_grid_sort@summary_table %>%
    as_tibble() %>%
    mutate_at(vars(-model_ids), list(as.numeric)) %>%
    arrange(desc(rmse)) %>%
    slice(1)
  
  other_params_not_tuned = best_gbm@model$model_summary %>%
    as_tibble() %>%
    select(-any_of(colnames(best_params)))
  
  best_params_with_metrics = best_params %>%
    bind_cols(other_params_not_tuned)
  
  #Clean out the h2o cluster.
  h2o.removeAll()
  
  #Reporting out Metrics
  out = list(best_params_with_metrics = best_params_with_metrics,
             worst_params = worst_params)
  
  return(out)
})

best_params = map(rocv_tuning, function(x) x[["best_params_with_metrics"]]) %>%
  bind_rows() %>%
  unite(col = "Param_String", col_sample_rate:sample_rate, remove = F)

most_frequent_best_params = best_params %>%
  count(Param_String) %>%
  filter(n == max(n))

best_params %>%
  semi_join(most_frequent_best_params, by = "Param_String") %>%
  select(col_sample_rate:sample_rate, number_of_trees, min_depth) %>%
  distinct()

library(knitr)
library(kableExtra)

best_params2 = best_params %>%
  select(col_sample_rate, learn_rate, sample_rate, rmse) 

kable(best_params2, format = "html") %>%
  kable_styling(full_width = FALSE,
                bootstrap_options = c("striped","condensed"))
```
