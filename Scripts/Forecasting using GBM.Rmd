---
title: "Building a High-Performing Forecasting Model using R and H2O"
author: "David Torgerson"
date: "3/31/2021"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 1
  editor_options:
    chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = F, warning = F)
```

# Introduction

This document expalins a high-performance regression model I built to forecast daily crimes
in Buffalo, NY. This was an exciting challenge that helped improve my data science skills
including: R Programming, data manipulation/cleaning, scalable machine learning with H2O, 
and Git callaboration. 

Unlike typical forecasting problems which are univariate in nature, I decided to tackle multivariate forecasting by building a single regression model that makes forecasts for 6 difference crime categories. I used the Gradient Boosted Machine (GBM) algorithm from the H2O library. To test and improve the model, I implemented rolling origin cross validation, hyperparameter tuning using grid search, and standard error metrics including Mean Absolute Scaled Error (MASE) and Root Mean Squared Error (RMSE).

# R Libraries

Package versions can be found at the bottom of the document under [Session Information]
```{r}
library(tidyverse)
library(rsample)
library(tis)
library(h2o)
library(DT)
library(lubridate)
```

# Reading in the Data

A CSV version of the data can be obtained here:  https://data.buffalony.gov/Public-Safety/Crime-Incidents/d6g9-xbgu 

```{r}
crime = read_csv("Data/Crime_Incidents.csv")
```

# Cleaning the Data

There were some minor issues with the data that needed to be fixed, inclduing:
- Convert dates from string to date types
- Standardize text fields
- Remove older data to unreliable data quality
- Remove crime categories with minimal observations

```{r}
crime_clean = crime %>%
  mutate(incident_datetime = mdy_hms(incident_datetime)) %>%
  mutate(incident_date = as.Date(incident_datetime)) %>%
  mutate(incident_type_primary = toupper(incident_type_primary)) %>%
  mutate(day_of_week = str_to_sentence(day_of_week)) %>%
  mutate(incident = case_when(
    incident_type_primary %in% c('THEFT OF SERVICES','THEFT OF VEHICLE', 'LARCENY/THEFT') ~ 'THEFT', 
    incident_type_primary %in% c('AGG ASSAULT ON P/OFFICER', 'AGGR ASSAULT') ~ 'ASSAULT',
    incident_type_primary %in% c('SEXUAL ABUSE', 'RAPE') ~ 'SEXUAL ABUSE/RAPE', 
    TRUE ~ incident_type_primary)
  ) %>%
  filter(incident_date >= '2015-01-01') %>%
  filter(incident %in% c("ASSAULT", "BURGLARY", "THEFT", "ROBBERY", "UUV", "SEXUAL ABUSE/RAPE"))
```

# Aggregating the data to daily crime counts

Now having clean data, I aggregated the data to include daily counts of crimes by category(incident)

```{r}
daily_incidents = crime_clean %>%
  count(incident_date, incident) %>%
  rename(crime_count = n) %>%
  arrange(incident, incident_date)

daily_incidents %>%
  count(incident)
```

Analysis of the aggregated data revealed the absence of dates for some incident types. I presume that missing dates implies zero crimes, so I imputed missing dates and assigned a value of 0 for the crime count.

```{r}
# Creating a sequence of dates to ensure there are no missing dates.
seq_dates = seq.Date(
  from = min(daily_incidents$incident_date),
  to = max(daily_incidents$incident_date),
  by = 1
) %>%
  enframe(name = NULL, value = "incident_date")

# Anything with a missing date will have a value of 0.
daily_incidents_complete = daily_incidents %>%
  split(.$incident) %>%
  map(., function(i) {
    left_join(
      x = seq_dates,
      y = i,
      by = "incident_date"
    ) %>%
      fill(incident, .direction = "downup") %>%
      replace_na(list(crime_count = 0))
  }) %>%
  bind_rows()

# visualization of daily incidents
daily_incidents_complete %>%
  ggplot(aes(x = incident_date, y = crime_count)) +
  facet_wrap(~incident, scales = "free_y") +
  geom_line()
```

# Feature engineering

Features that were tested in the model are:

- Date-based features for a given date (i.e. month, year, week, weekday)
- US holiday indicators

```{r}
# Function to "tag" features to easily include/exclude indicators from the model
tag_columns = function(x, prefix) {
  paste(prefix, x, sep = "_")
}

# Date-based features
date_features = daily_incidents_complete %>%
  mutate(month = lubridate::month(incident_date)) %>%
  mutate(year = lubridate::year(incident_date)) %>%
  mutate(week = lubridate::week(incident_date)) %>%
  mutate(weekday = lubridate::wday(incident_date))

# US Bank holidays 
bank_holidays = tis::holidays(years = seq(2015, 2021, 1)) %>%
  enframe(name = 'Holiday', value = 'Date') %>%
  mutate(Date = as.Date(as.character(Date), '%Y%m%d')) %>%
  mutate(is_Holiday = 1)

# Joining holiday dummy features with original data
complete_features = left_join(
  date_features,
  bank_holidays,
  by = c("incident_date" = "Date")
) %>%
  replace_na(list(Holiday = "None")) %>%
  spread(Holiday, is_Holiday) %>% 
  mutate(Easter = isEaster(incident_date)) %>%
  mutate(Easter = ifelse(Easter == TRUE, 1, 0)) %>%
  select(-None) %>%
  replace(is.na(.),0) 

complete_features_with_tags = complete_features %>%
  rename_at(vars(month:weekday), list(~tag_columns(x = ., prefix = 'Date'))) %>%
  rename_at(vars(Christmas:Easter), list(~tag_columns(x = ., prefix = 'Holiday'))) 

str(complete_features_with_tags)
```

# Preparing cross validation

In order to compare the expected performance of the model, I implemented rolling origin cross validation (ROCV). 

```{r}
rocv = rolling_origin(
  data = complete_features_with_tags %>% spread(incident, crime_count),
  initial = floor(365.25*5),
  assess = 14,
  cumulative = TRUE,
  skip = 14 
)
```

# Preparing Train and Test sets within ROCV

Next, we need to extract the training and testing sets from each ROCV iteration.

```{r}
train_test_splits = map(rocv$splits, function(split) {
  train = analysis(split) %>% gather(incident, crime_count, -contains("Date"), -contains("Holiday"))
  test = assessment(split) %>% gather(incident, crime_count, -contains("Date"), -contains("Holiday"))
  out = list(train = train, test = test)
  return(out)
})

train_test_splits[1]
```

# Model Implementation

Next, I used H2O to implement the forecasting model using the gradient-boosted machine (GBM). This model performed the best against standard linear regression and random forest models. Separate GBMs are fit within each iteration of the ROCV and then performance is calculated with the testing data.

To evaluate the performance of the GBM, I compared it to a "simple model" based on naive predictions. Our simple model states that we'll use the actual values from the last 14 days to predict the next 14 days. The idea is that the GBM model should perform better than this naive model.

```{r}
# Initialize H2O
h2o.init()
h2o.no_progress()
```

```{r}
rocv_models = map(train_test_splits, function(i) {
  
  # train/test split
  train = i$train %>% mutate(incident = as.factor(incident))
  test = i$test %>% mutate(incident = as.factor(incident))
  
  # convert train/test to H2O format
  train_h2o = as.h2o(train)
  test_h2o = as.h2o(test)
  
  # list of features
  features = train %>%
    select(-crime_count, -incident_date) %>%
    colnames()
  
  # target variable
  target = "crime_count"
  
  # GBM using H2O
  model = h2o.gbm(
    x = features,
    y = target,
    training_frame = train_h2o,
    validation_frame = test_h2o,
    ntrees = 50,
    learn_rate = 0.1,
    max_depth = 5,
    categorical_encoding = "AUTO",
    distribution = "poisson" 
  )
  
  # Extracting predictions
  predictions = h2o.predict(model, newdata = test_h2o) %>%
    as.vector()
  
  # Simple Predictions Calculation
  simple_model = train %>%
    arrange(incident, incident_date) %>% 
    group_by(incident) %>%
    slice(tail(row_number(), 14)) %>%
    ungroup() %>%
    mutate(incident_date = incident_date + days(14)) %>%
    select(incident_date, incident, Simple = crime_count)
  
  # MASE Calculation
  MASE_by_incident = test %>%
    mutate(Predictions = predictions) %>% 
    inner_join(simple_model, by = c("incident","incident_date")) %>%
    group_by(incident) %>%
    summarise(
      GBM_MAE = mean(abs(Predictions - crime_count)),
      Simple_MAE = mean(abs(Simple - crime_count))
    ) %>%
    mutate(MASE = GBM_MAE/Simple_MAE)
  
  #Clean out the h2o cluster.
  h2o.removeAll()
  
  return(MASE_by_incident)
})
```

# Expected model performance

```{r}
rocv_df = map2(rocv_models, seq_along(rocv_models), function(model, idx) {
  out = model %>% mutate(rocv_idx = idx)
  return(out)
}) %>%
  bind_rows()

rocv_df %>%
  group_by(incident) %>%
  filter(MASE != 'Inf') %>%
  summarise(N = n(),
            Avg_MASE = mean(MASE),
            SD_MASE = sd(MASE),
            Lower_CI = t.test(MASE)$conf.int[1],
            Upper_CI = t.test(MASE)$conf.int[2])
```

# Conclusion

We can conclude from the final output above, that our model performed better than just our simple model in 5/6 of the incidents. 

# Next Steps

Next steps in this analysis would include hyperparameter tuning and looking to see if there is any difference in crime on holidays.

# Session Information {#session-info}

```{r}
sessionInfo()
```