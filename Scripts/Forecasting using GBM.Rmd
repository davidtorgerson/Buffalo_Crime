---
title: "Forecasting using GBM"
author: "David Torgerson & Danny Morris"
date: "3/19/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This project looks at trying to forecast crime rates in Buffalo, NY. The data used for this analysis can be located here:https://data.buffalony.gov/Public-Safety/Crime-Incidents/d6g9-xbgu . The analysis involves multiple data science practices including, data extraction, data cleaning, model building and model tuning. 

# Libraries Needed
```{Libraries}
library(tidyverse)
library(lubridate)
library(stringr)
library(tibble)
library(purrr)
library(rsample)
library(tis)
```

# Reading in the Data

For this particular analysis, I had downloaded the data as an excel file from the buffalony.gov website and saved it in a memorable folder.

```{Data}
crime = read_csv("Data/Crime_Incidents.csv")
```

# Cleaning the Data

There were some minor issues with the data that needed to be fixed. Some dates needed to be corrected, missing values filled in, need consistent formatting throughout a variable. After the initial round of cleaning, we noticed that there were still some problems. We had some outlier years that needed to be removed, needed to standardize the incident variables, and then remove any incidents that have very few observations.

```{Data Cleaning}

crime_clean = crime %>%
  mutate(address_1 = toupper(address_1)) %>%
  mutate(incident_datetime = mdy_hms(incident_datetime)) %>%
  mutate(incident_type_primary = toupper(incident_type_primary)) %>%
  mutate(day_of_week = str_to_sentence(day_of_week))

crime_since_2009 = crime_clean %>%
  filter(year(incident_datetime) >= '2009') %>%
  mutate(incident = case_when(
    incident_type_primary %in% c('THEFT OF SERVICES','THEFT OF VEHICLE') ~ 'THEFT', #Aggregating crimes
    incident_type_primary %in% c('AGG ASSAULT ON P/OFFICER', 'AGGR ASSAULT') ~ 'ASSAULT', #Aggregating crimes
    TRUE ~ incident_type_primary)) %>%
  filter(!incident %in% c('BREAKING & ENTERING','HOMICIDE','CRIM NEGLIGENT HOMICIDE',
                       'MANSLAUGHTER','OTHER SEXUAL OFFENSE','SEXUAL ASSAULT')) 
```

# Prepping the data for modeling

Once the data has been cleaned, we started prepping the data for modeling. We had to make sure there was no skip in dates and map a dates data frame to our clean data so we can make holiday indicator features. These holiday indicators will be used to determine feature importance.
```{Data Prep}
seq_dates = seq.Date(
  from = as.Date("2009-01-01"),
  to = as.Date("2021-01-24"),
  by = 1) %>%
  enframe(name = NULL, value = "incident_date")

all_daily_incidents = daily_incident_counts %>%
  split(.$incident) %>%
  map(., function(i) {
    left_join(
      x = seq_dates,
      y = i,
      by = "incident_date"
    ) %>%
      fill(incident, .direction = "downup") %>%
      replace_na(list(n = 0))
  }) %>%
  bind_rows()
#The code above splits the dataframe by incidents map the dates together
#Filling in all the missing dates with a count of 0
#Assumption: If date is missing from original data, that incident did not occur (count of 0)

#Extracting the other date-based data features
complete_daily_incidents = all_daily_incidents %>%
  mutate(month = month(incident_date)) %>%
  mutate(year = year(incident_date)) %>%
  mutate(week = week(incident_date)) %>%
  mutate(weekday = wday(incident_date)) %>%
  rename(crime_count = n)

#Getting Bank holidays from 2009 to 2021 to incorporate
bank_holidays = holidays(seq(2009,2021, 1)) %>%
  enframe(name = 'Holiday', value = 'Date') %>% #Creates df out of vector
  mutate(New_Date = as.Date(as.character(Date), '%Y%m%d')) %>% #Changing YYYYMMDD to YYYY-MM-DD
  mutate(is_Holiday = 1) #Labels all holidays with a value of 1

#Joining with crime data and creating holiday dummy variables.
complete_incidents_with_holidays = left_join(complete_daily_incidents,
                                             bank_holidays,
                                             by = c("incident_date" = "New_Date")) %>%
  select(-Date) %>%
  mutate(Holiday = ifelse(is.na(Holiday), "None",Holiday)) %>%
  spread(Holiday, is_Holiday) %>% #Use this to not need multiple mutate functions
  mutate(Easter = isEaster(incident_date)) %>%
  mutate(Easter = ifelse(Easter == TRUE, 1, NA)) %>%
  select(-None) %>%
  replace(is.na(.),0) #Replacing all NA values with 0
  
#Adding a 'Holidays' tag to filter out Holidays to compare models
tag_columns = function(x, prefix) {
  paste(prefix, x, sep = "_")
}

complete_incidents_with_holiday_tag = complete_incidents_with_holidays %>%
  rename_at(vars(-incident_date:-weekday), list(~tag_columns(x = ., prefix = 'Holiday')))

recent_daily_incidents = complete_incidents_with_holiday_tag %>% #Was complete_daily_incidents
  filter(incident_date >= '2015-01-01')
```

# Training and Testing Data

When building a forecasting model, we needed to separate out a training and testing data set. 

```{Training/Testing data}

recent_daily_incidents_wide = recent_daily_incidents %>%
  spread(incident, crime_count)

rocv_by_crime2 = recent_daily_incidents_wide %>%
  rolling_origin(
    data = .,
    initial = (365*4),
    assess = 14,
    cumulative = TRUE,
    skip = 14 
  )

train_test_splits = map(rocv_by_crime2$splits, function(split){
  train = analysis(split) %>%
    gather(incident, crime_count, -incident_date, -month, -year, -week, -weekday, -Holiday_Christmas:-Holiday_Easter)

  test = assessment(split) %>%
    gather(incident, crime_count, -incident_date, -month, -year, -week, -weekday, -Holiday_Christmas:-Holiday_Easter)
  
  out = list(train = train, test = test)
  
  return(out)
})

```

# Model Implementation

Next, we used h2o to build our gradient-boosted machine (GBM) model. This model performed the best against standard linear regression and random forest models. The GBM is being tested against a "simple model". Our Simple model states that we'll use the information from the last 14 days to make a prediction about what will happen next. The t-test shows that our model performs better on 6/9 of the incidents. 

```{Model Building}

library(h2o)

h2o.init()

rocv_models = map(train_test_splits, function(x){
  
  train = x$train %>%
    mutate_at(vars(month:incident), list(as.factor)) #Converting crimes to categories
  
  test = x$test %>%
    mutate_at(vars(month:incident), list(as.factor)) #Converting crimes to categories
  
  features = train %>%
    select(-crime_count, -incident_date) %>%
    colnames()
  
  train_h2o = as.h2o(train)
  
  test_h2o = as.h2o(test)
  
  target = "crime_count"

  model = h2o.gbm(
    x = features,
    y = target,
    training_frame = train_h2o,
    validation_frame = test_h2o,
    ntrees = 50, #How many trees do you want the model to build, Default set at 50
    learn_rate = 0.1, #Default set at 0.1
    max_depth = 5, #Default set at 5
    categorical_encoding = "AUTO", #Default set at AUTO
    distribution = "poisson" #Used for non-negative count data
)
  
  #Simple Predictions Calculation
  simple_model = train %>%
    arrange(incident, incident_date) %>% #Sorting by incident and date
    group_by(incident) %>%
    slice(tail(row_number(), 14)) %>% #Grab the last 14 rows of each incident
    ungroup() %>%
    mutate(incident_date = incident_date + days(14)) %>% #Adding 14 days to merge with testing data
    select(incident_date, incident, Simple = crime_count)
  
  #Predictions from GBM Model
  predictions = h2o.predict(model, newdata = test_h2o) %>%
    as.vector()
  
  #MASE Calculation
  MASE_by_incident = test %>%
    mutate(Predictions = predictions) %>% #Adding predictions from gbm 
    inner_join(simple_model, by = c("incident","incident_date")) %>% #adding simple predictions
    group_by(incident) %>%
    summarise(
      GBM_MAE = mean(abs(Predictions - crime_count)), #Getting Mean Average Error for GBM
      Simple_MAE = mean(abs(Simple - crime_count)) #Getting Mean Average Error for Simple
    ) %>%
    mutate(MASE = GBM_MAE/Simple_MAE) #Calculating MASE
  
  #Clean out the h2o cluster.
  h2o.removeAll()
  
  #Reporting out Metrics
  out = MASE_by_incident
  
  return(out)
})

rocv_df = map2(rocv_models, seq_along(rocv_models), function(model, idx){ #Creates a df of all the rocv models and labeling them with an iteration number
  
  out = model %>% mutate(rocv_idx = idx)
  
  return(out)
}) %>%
  bind_rows()

rocv_df %>%
  group_by(incident) %>%
  filter(MASE != 'Inf') %>%
  summarise(N = n(),
            Avg_MASE = mean(MASE),
            SD_MASE = sd(MASE),
            Lower_CI = t.test(MASE)$conf.int[1],
            Upper_CI = t.test(MASE)$conf.int[2])
```